---
title: "[Deep Learning] CNN (Convolutional Neural Network)"
excerpt: "컴퓨터 비전의 핵심 기술, CNN의 원리와 특징에 대해 알아보자"

tags:
  - python
  - AI engineering
  - deep learning
  - CNN
  - computer vision

toc: true
toc_sticky: true

date: 2025-07-29
last_modified_at: 2025-07-29
categories: 
  - deeplearning
---

## CNN (Convolutional Neural Network)이란?

**CNN**은 주로 이미지 인식과 같은 **컴퓨터 비전 분야**의 데이터를 분석하기 위해 사용되는 인공 신경망의 한 종류입니다. 핵심은 **합성곱(Convolution) 연산**을 사용한다는 점입니다.

> **핵심 아이디어**: 이미지의 공간적 구조를 보존하면서 효율적으로 특징을 추출하는 것

---

## 컴퓨터는 이미지를 어떻게 볼까?

### 흑백 이미지
- **단일 채널** 사용
- 각 픽셀은 **0~255 범위**의 하나의 값만 가짐
- 명암의 정도를 나타냄

### 컬러 이미지  
- **RGB 3개 채널** 사용
- 각 채널은 **0~255 값**을 가짐
- 세 채널의 조합으로 다양한 색상 표현

```
예시: 224×224 컬러 이미지
- 입력 형태: (224, 224, 3)
- 총 픽셀 수: 224 × 224 × 3 = 150,528개 값
```

---

## FCN (Fully Connected Network)의 한계

전통적인 FCN으로 이미지를 처리할 때 발생하는 문제들:

- **파라미터 수의 폭발적 증가** → 과적합 위험 및 계산 비용 증가
- **확장성 부족** → 대용량 이미지 처리의 어려움
- **인접 픽셀 관계 반영 불가** → 공간적 정보 손실
- **1차원 벡터 변환** → 패턴 학습의 어려움

> **문제점**: 224×224 이미지를 FCN으로 처리하면 첫 번째 레이어만 해도 150,528개의 가중치가 필요!

---

## 사람은 어떻게 이미지를 인식하는가?

### 고양이 뉴런 실험 (Hubel & Wiesel, 1959)
- 특정 뉴런이 **수직, 수평, 대각선**에 각각 반응하는 것을 관측
- **전체 뉴런이 아닌 특정 뉴런**이 활성화되는 현상 발견
- 각 뉴런의 **전문화된 역할** 존재 확인

### CNN의 계층적 인식 과정
1. **1단계**: 선과 같은 **단순한 특징** 잡아내기
2. **2단계**: 모서리, 경계선 등 **중간 복잡도 특징** 학습
3. **3단계**: 부분과 전체를 연결하는 **복잡한 특징** 인식

> **인간의 시각 인식과 유사**: 단순한 요소부터 복잡한 패턴까지 점진적 학습

---

## CNN의 역사

### LeNet-5 (1998)
- **얀 르쿤(Yann LeCun)**에 의해 개발
- CNN의 **최초 구현 모델**
- 손글씨 인식에 혁신적인 성과 달성

### 주요 발전 과정
- **2012**: AlexNet - ImageNet 대회에서 혁신적 성과
- **2014**: VGGNet - 깊은 네트워크 구조
- **2015**: ResNet - 잔차 연결로 깊은 네트워크 학습 가능
- **2017**: Transformer - Attention 메커니즘 도입

---

## CNN의 핵심 특징

### 1. 부분 연결 (Local Connectivity)
- 전체 이미지를 한 번에 분석하지 않음
- **작은 지역부터 점진적 학습**
- FCN과 차별화되는 핵심 특징

### 2. 가중치 공유 (Weight Sharing)
- **동일한 필터**를 이미지 전체에 적용
- 파라미터 수 대폭 감소
- **계산 효율성 향상**

### 3. 계층적 특성 (Hierarchical Feature Learning)
- **하위 레이어**: 단순한 특징 (선, 모서리)
- **상위 레이어**: 복잡한 특징 (얼굴, 객체)
- 점진적으로 추상화된 특징 학습

> **핵심**: 지역적 패턴 → 전역적 패턴으로의 점진적 학습

---

## 합성곱 연산의 핵심 요소

### 1. 커널 (Kernel) / 필터 (Filter)
- **커널 사이즈**: CNN에서 사용하는 필터 행렬의 크기
- 일반적으로 3×3, 5×5, 7×7 등의 크기 사용
- 작은 커널: 세밀한 특징 추출
- 큰 커널: 넓은 영역의 특징 포착

```
커널 크기별 특징:
- 3×3: 세밀한 특징, 계산 효율적
- 5×5: 중간 수준 특징
- 7×7: 넓은 영역 특징, 계산 비용 높음
```

### 2. 스트라이드 (Stride)
- 합성곱 필터의 **이동 간격**
- 몇 칸씩 이동할지 설정하는 것 (보통 1이나 2로 설정)
- **스트라이드가 커지면 출력 맵의 크기는 작아짐**

#### 스트라이드의 특징
- 합성곱 연산의 복잡도를 줄임
- 너무 큰 스트라이드는 정보 누락의 위험
- 스트라이드로 건너뛰더라도 아예 무시하는 픽셀은 없음

### 3. 패딩 (Padding)
- 가장자리일수록 특성에 반영되는 횟수가 적은 특징을 이용
- 입력 데이터 가장자리에 0이나 다른 값을 추가하여 크기를 확장하는 과정

#### 패딩의 목적
- 연산 시 이미지 가장자리 정보 손실 방지
- 출력 크기 조정

#### 패딩의 종류

**Valid Padding**
- 패딩을 사용하지 않음 (no padding)
- 출력 크기 줄어듦
- 계산량 감소
- 가장자리 연산 참여 횟수가 적어 가장자리 정보 손실 위험
- 간단한 특징 학습에 유리

**Same Padding**
- 출력 맵의 크기와 입력 맵의 크기가 같아지도록 필요한 만큼 패딩 추가
- CNN 모델에서 가장 널리 사용

**Full Padding**
- 출력 크기가 입력 맵보다 커지도록 충분한 패딩 추가
- 더 넓은 영역을 학습할 때 사용

---

## 합성곱 레이어 (Convolutional Layer) 상세 분석

### 합성곱 연산의 수학적 특성

#### 1. 선형성 (Linearity)
- **합성곱 연산은 선형적**입니다
- 입력값과 필터값의 **선형 조합**만 포함
- 수학적으로: `f(x + y) = f(x) + f(y)`, `f(ax) = af(x)` 성질 만족

#### 2. 비선형성 추가의 필요성
- **합성곱 연산 후 활성화 함수를 적용**하여 비선형성을 추가
- **ReLU 함수**가 가장 인기 있고 널리 사용됨
- 비선형성을 추가하지 않으면 **선형 변환에 불과**하여 복잡한 데이터 패턴을 학습할 수 없음

> **핵심**: 선형 연산 + 비선형 활성화 함수 = 강력한 특징 추출 능력

### 출력 맵 크기 계산 공식

```
출력 맵 크기 = (입력 크기 - 필터 크기 + 2 × 패딩) / 스트라이드 + 1
```

#### 계산 예시
- **입력**: 32×32 이미지
- **필터**: 3×3 커널
- **패딩**: 1 (Same padding)
- **스트라이드**: 1

```
출력 크기 = (32 - 3 + 2 × 1) / 1 + 1 = 32
```

### 활성화 함수의 역할

#### 1. ReLU (Rectified Linear Unit)
- **가장 널리 사용**되는 활성화 함수
- **계산 효율적**: `f(x) = max(0, x)`
- **기울기 소실 문제 완화**
- **희소성(sparsity) 유도**

#### 2. 기타 활성화 함수
- **Sigmoid**: 이진 분류에서 출력층에 사용
- **Softmax**: 다중 분류에서 출력층에 사용
- **Leaky ReLU**: ReLU의 개선 버전

---

## CNN의 주요 레이어

### 1. 합성곱 레이어 (Convolutional Layer)
- **핵심 연산**: 합성곱 연산 수행
- **목적**: 특징 추출
- **출력**: 특징 맵 (Feature Map)

### 2. 풀링 레이어 (Pooling Layer)
- **목적**: 차원 축소 및 계산량 감소
- **종류**: Max Pooling, Average Pooling, Min Pooling
- **효과**: 과적합 방지, 계산 효율성 향상

### 3. 완전 연결 레이어 (Fully Connected Layer)
- **목적**: 최종 분류
- **입력**: 평탄화된 특징 벡터
- **출력**: 클래스 확률

### 4. 활성화 함수 (Activation Function)
- **ReLU**: 가장 널리 사용, 계산 효율적
- **Sigmoid**: 이진 분류에서 출력층에 사용
- **Softmax**: 다중 분류에서 출력층에 사용

---





