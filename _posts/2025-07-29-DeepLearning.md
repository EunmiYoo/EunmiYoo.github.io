---
title: "[Deep Learning] 딥러닝의 핵심 개념과 최적화"
excerpt: "딥러닝 모델의 최적화 방법과 핵심 개념들을 체계적으로 알아보자"

tags:
  - python
  - AI engineering
  - deep learning
  - machine learning
  - neural networks

toc: true
toc_sticky: true

date: 2025-07-29
last_modified_at: 2025-07-29
categories: 
  - deeplearning
---
퍼셉트론(Perceptron)
1. 단일 퍼셉트론 (Single Perceptron),
단일 퍼셉트론은 입력값에 가중치를 곱해 합산한 뒤, 그 결과가 임계치 이상이면 1, 미만이면 0을 출력하는 아주 단순한 구조입니다. 단점으로는 선형 결정 경계만 만들 수 있어서, 비선형 패턴(XOR 문제)을 전혀 학습하지 못합니다.
2. 다층 퍼셉트론 (Multi‑Layer Perceptron),
위의 선형 한계를 극복하기 위해, 여러 개의 퍼셉트론(뉴런) 층을 쌓아 은닉층(hidden layer)을 도입한 구조입니다.
1. 비선형 활성화 함수를 은닉층마다 적용해, 입력이 복잡하게 뒤얽힌 패턴까지도 모델이 분해·학습할 수 있게 함,
2. 층(layer)을 깊게 쌓음으로써, 단순 직선이 아닌 다차원 곡선·곡면 형태의 비선형 결정 경계를 표현,
3. 결과적으로 XOR과 같은 대표적 비선형 문제 뿐 아니라, 이미지·음성·텍스트의 복잡한 특성도 포착하며 예측 성능을 크게 향상시킴


신경망 레이어는 입력층-은닉층-출력층으로 나뉘며 은닉층의 여부에 따라 단층/다층 퍼셉트론으로 구분 지을 수 있다. 은닉층에서는 비선형적 데이터의 복잡한 특성이나 패턴을 추출하고 학습한다. 여기서 정교한 모델 구축이 이루어진다. 은닉층에서 각 층의 개별 뉴런은 가중합 연산을 통해 출력된 값을 활성화 함수로 넘긴 출력을 다음 레이어로 넘겨 이어간다. 은닉층을 거친 최초의 변수 값은 최종적으로 출력층을 통해 나온다. 이 모델의 예측값인 y_pred를 MSE를 사용하여 실젯값 Y와 함께 함수로 정의하면 MSE = 1/n 시그마 1/2(y_pred-Y)제곱으로 나타낸다. 이 손실함수 L에 대하여 w1이라는 가중치를 구하고 싶을 경우, 이는 dL/dw1이라는 미분 방정식을 사용하게 되는데, 이를 체인룰로 전개시켜서 x변수 입력이 선형함수와 시그모이드 함수로 순차적으로 입력되는 과정을 역전파시켜서 w1이 변화할 시 L이라는 손실함수를 얼마나 변화시키는지를 나타낼 수 있다. 따라서 은닉층은 신경망에서 가장 중요한 역할을 한다고 볼 수 있다.


순전파(Foward Propagation) : 입력 데이터가 각 레이어를 통과해서 출력층까지 가는 과정. 출력된 값은 예측값이며 실제값과 비교하여 손실함수를 최소화한다.


역전파 딥러닝 모델이 잘 학습되었다는 증거는 예측 결과값이 실제 값과 가까우며 오차가 최소화된 상태 손실함수의 값을 가능한한 최소로 줄이는 것이 목표이고 초기 가중치가 랜덤으로 설정되기에 처음에는 실제와 예측 값 사이엔 큰 오차가 존재하고 그 오차를 줄이기 위한 과정이 바로 역전파(Backpropagation)이다. 역전파는 신경망의 출력과 목표값 사이의 오차를 최소화 하기 위해 네트워크에 가중치를 조정하는 과정이라 할 수 있다. 역전파는 출력에서 발생한 오차값을 신경망을 거슬러 올라가며 각 층의 가중치가 오차에 얼마나 기여하는지 계산하고 이 정보를 사용하여 가중치를 업데이트 한다. 이 과정을 통해 신경망은 오차를 최소화 하는 방향으로 점차 개선된다. 역전파 과정에서는 경사 하강법를 통해 손실함수를 최소화 하는 방향으로 가중치를 조절하게된다.

## 딥러닝 모델의 최적화가 어려운 이유

딥러닝 모델을 최적화하는 과정에서 발생하는 주요 문제들과 그 원인을 살펴보겠습니다.

### 1. 모델의 비선형성

**활성화 함수의 비선형성**으로 인해 손실 함수의 지형이 매우 복잡해집니다.

- **비볼록(Non-convex) 지형** 형성
- 복잡하고 울퉁불퉁한 표면
- **전역 최솟값을 찾기 어려움**

### 2. 고차원성과 과적합

**많은 개수의 파라미터**를 포함하여 차원이 증가하면서 발생하는 문제:

- **과적합 위험 증가**
- 과적합: 학습 데이터에 너무 과도하게 맞춰져서 새로운 데이터에 대한 예측 성능 저하
- 모델의 일반화 능력 저하

### 3. 그래디언트 소실 (Gradient Vanishing)

**시그모이드 함수**의 특성으로 인한 문제:

- 0과 1 사이를 벗어나면 기울기가 0에 수렴
- **연쇄 법칙(체인룰)**에 따라 여러 층을 거치면서 작은 기울기 값들이 계속 곱해짐
- 앞쪽(입력층에 가까운) 층으로 갈수록 기울기가 기하급수적으로 작아짐
- **가중치 업데이트가 거의 이루어지지 않음**

### 4. 하이퍼파라미터의 민감성

**사용자가 사전에 설정해야 할 값**으로, 성능 최적화에 매우 중요:

- 설정 변수의 적절한 조정이 최적의 모델을 위해 필수
- 잘못된 설정 시 모델 성능 크게 저하

---
##딥러닝 모델링 시 **필수적으로 고려할 사항

## 하이퍼파라미터 (Hyperparameter)

사용자가 사전에 설정해야 하는 값입니다.

### 1. 배치 크기 (Batch Size)

**한 번의 학습에 사용되는 샘플의 수**

- **메모리 사용량** 때문에 배치 사이즈를 한 번에 올릴 수 없음
- **학습 속도**에 직접적인 영향


### 2. 학습률 (Learning Rate, α)

**가중치를 어느 정도로 업데이트할지 결정하는 매개변수**

```
새로운 가중치 = 이전 가중치 - 학습률(α) × 기울기
```

**문제점:**
- **너무 낮으면**: 학습이 지나치게 느려짐
- **너무 높으면**: 손실함수가 최적값 주변에서 요동치거나 발산

### 3. 에폭 수 (Epoch)

**전체 훈련 데이터셋이 네트워크를 통과하는 횟수**

- **너무 많으면**: 학습 시간 증가, 과적합 초래
- **너무 적으면**: 학습 부족으로 성능 저하

### 4. 옵티마이저 (Optimizer)

**손실함수를 최소화하기 위해 모델의 가중치를 어떻게 업데이트할지 결정**

- 확률적 경사하강법 (SGD)
- Adam
- Momentum


---

## 활성화 함수 (Activation Function)

**뉴런에서 입력신호의 총합을 받아 출력신호로 변환하는 역할**

### 1. 시그모이드 함수 (Sigmoid)

**초기 신경망에서 널리 사용**

- **출력 범위**: 0~1
- **문제점**: 0과 1을 벗어난 부분에서 **그래디언트 소실 문제**가 심함

### 2. 하이퍼볼릭 탄젠트 함수 (tanh)

**시그모이드의 변형함수**

- **출력 범위**: -1~1로 확장
- **장점**: 학습 초기에 빠른 수렴에 도움
- **문제점**: 여전히 그래디언트 소실 문제 존재

### 3. ReLU 함수 (Rectified Linear Unit)

**현대 신경망에서 가장 인기 있는 함수**

- **양수 입력**: 그대로 출력, 그래디언트 감소하지 않고 효과적으로 전달
- **음수 입력**: 0을 출력, 그래디언트도 0, 가중치 업데이트 제한

### 4. Leaky ReLU 함수

**ReLU 함수의 보완 버전**

- **음수 입력**: 아주 작은 기울기 유지
- **장점**: ReLU의 장점을 유지하면서 그래디언트 소실 문제 완화

---

## 경사하강법 (Gradient Descent)

**최적화 알고리즘의 일종**으로, 여러 번 반복하여 최적의 가중치를 찾아나가는 과정

### 용어 정리

- **배치 (Batch)**: 한 번의 학습 단계에 사용되는 학습 데이터의 묶음
- **에폭 (Epoch)**: 전체 학습 데이터셋을 한 번 학습하는 과정

### 1. 배치 경사하강법 (Batch Gradient Descent)

**기울기 계산에 전체 데이터셋 사용**

- **장점**: 모든 데이터를 보고 최적을 찾음
- **단점**: 대규모 데이터셋에서는 **계산 비용이 높음**

### 2. 확률적 경사하강법 (Stochastic Gradient Descent, SGD)

**각 반복에서 무작위로 선택된 하나의 데이터 샘플 사용**

- **장점**: 배치보다 계산 속도 빠름
- **단점**: 경사의 추정이 불안정해서 학습 과정이 매끄럽지 않음

### 3. 미니배치 경사하강법 (Mini-batch Gradient Descent)

**배치 경사하강과 확률적 경사하강의 절충안**

- **장점**: 
  - 높은 메모리 효율
  - 빠른 계산 속도
  - 지역 최솟값에 갇힐 가능성 감소
- **단점**: 배치 크기 등 하이퍼파라미터에 매우 민감

---

## 출력 함수 (Output Function)

### 회귀 문제
- **출력**: 연속적인 값
- **예시**: 집값 예측, 온도 예측

### 분류 문제

#### 이진 분류
- **함수**: 시그모이드 함수
- **출력**: 0과 1 사이의 확률값

#### 다중 분류
- **함수**: 소프트맥스 함수 (Softmax)
- **출력**: 각 클래스에 대한 확률 분포

---

## 손실 함수 (Loss Function)

**모델이 실제값을 얼마나 잘 예측하는지를 측정**하는 함수

### 목표
예측값과 실제값의 차이인 손실함수의 값을 **가능한 작게** 만들어야 함

### 종류별 손실 함수

#### 1. 회귀 문제
- **MSE (Mean Squared Error, 평균제곱오차)**
- 숫자를 예측하는 문제에 사용

#### 2. 분류 문제

**이진 분류**
- **이진 교차 엔트로피 (Binary Cross Entropy)**
- 두 가지 클래스 중 하나로 분류하는 문제

**다중 분류**
- **교차 엔트로피 (Cross Entropy)**
- 세 가지 이상의 클래스 중 하나로 분류하는 문제

---


 
