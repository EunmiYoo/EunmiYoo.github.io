---
title: "[Deep Learning] 딥러닝의 핵심 개념과 최적화"
excerpt: "딥러닝 모델의 최적화 방법과 핵심 개념들을 체계적으로 알아보자"

tags:
  - python
  - AI engineering
  - deep learning
  - machine learning
  - neural networks

toc: true
toc_sticky: true

date: 2025-07-29
last_modified_at: 2025-07-29
categories: 
  - deeplearning
---

## 딥러닝 모델의 최적화가 어려운 이유

딥러닝 모델을 최적화하는 과정에서 발생하는 주요 문제들과 그 원인을 살펴보겠습니다.

### 1. 모델의 비선형성

**활성화 함수의 비선형성**으로 인해 손실 함수의 지형이 매우 복잡해집니다.

- **비볼록(Non-convex) 지형** 형성
- 복잡하고 울퉁불퉁한 표면
- **전역 최솟값을 찾기 어려움**

### 2. 고차원성과 과적합

**많은 개수의 파라미터**를 포함하여 차원이 증가하면서 발생하는 문제:

- **과적합 위험 증가**
- 과적합: 학습 데이터에 너무 과도하게 맞춰져서 새로운 데이터에 대한 예측 성능 저하
- 모델의 일반화 능력 저하

### 3. 그래디언트 소실 (Gradient Vanishing)

**시그모이드 함수**의 특성으로 인한 문제:

- 0과 1 사이를 벗어나면 기울기가 0에 수렴
- **연쇄 법칙(체인룰)**에 따라 여러 층을 거치면서 작은 기울기 값들이 계속 곱해짐
- 앞쪽(입력층에 가까운) 층으로 갈수록 기울기가 기하급수적으로 작아짐
- **가중치 업데이트가 거의 이루어지지 않음**

### 4. 하이퍼파라미터의 민감성

**사용자가 사전에 설정해야 할 값**으로, 성능 최적화에 매우 중요:

- 설정 변수의 적절한 조정이 최적의 모델을 위해 필수
- 잘못된 설정 시 모델 성능 크게 저하

---

## 하이퍼파라미터 (Hyperparameter)

딥러닝 모델링 시 **필수적으로 고려할 사항**으로, 사용자가 사전에 설정해야 하는 값입니다.

### 1. 배치 크기 (Batch Size)

**한 번의 학습에 사용되는 샘플의 수**

- **메모리 사용량** 때문에 배치 사이즈를 한 번에 올릴 수 없음
- **학습 속도**에 직접적인 영향
- 일반적으로 32, 64, 128, 256 등 사용

### 2. 학습률 (Learning Rate, α)

**가중치를 어느 정도로 업데이트할지 결정하는 매개변수**

```
새로운 가중치 = 이전 가중치 - 학습률(α) × 기울기
```

**문제점:**
- **너무 낮으면**: 학습이 지나치게 느려짐
- **너무 높으면**: 손실함수가 최적값 주변에서 요동치거나 발산

### 3. 에폭 수 (Epoch)

**전체 훈련 데이터셋이 네트워크를 통과하는 횟수**

- **너무 많으면**: 학습 시간 증가, 과적합 초래
- **너무 적으면**: 학습 부족으로 성능 저하

### 4. 옵티마이저 (Optimizer)

**손실함수를 최소화하기 위해 모델의 가중치를 어떻게 업데이트할지 결정**

- 확률적 경사하강법 (SGD)
- Adam
- Momentum
- RMSprop 등

---

## 활성화 함수 (Activation Function)

**뉴런에서 입력신호의 총합을 받아 출력신호로 변환하는 역할**

### 1. 시그모이드 함수 (Sigmoid)

**초기 신경망에서 널리 사용**

- **출력 범위**: 0~1
- **문제점**: 0과 1을 벗어난 부분에서 **그래디언트 소실 문제**가 심함

### 2. 하이퍼볼릭 탄젠트 함수 (tanh)

**시그모이드의 변형함수**

- **출력 범위**: -1~1로 확장
- **장점**: 학습 초기에 빠른 수렴에 도움
- **문제점**: 여전히 그래디언트 소실 문제 존재

### 3. ReLU 함수 (Rectified Linear Unit)

**현대 신경망에서 가장 인기 있는 함수**

- **양수 입력**: 그대로 출력, 그래디언트 감소하지 않고 효과적으로 전달
- **음수 입력**: 0을 출력, 그래디언트도 0, 가중치 업데이트 제한

### 4. Leaky ReLU 함수

**ReLU 함수의 보완 버전**

- **음수 입력**: 아주 작은 기울기 유지
- **장점**: ReLU의 장점을 유지하면서 그래디언트 소실 문제 완화

---

## 경사하강법 (Gradient Descent)

**최적화 알고리즘의 일종**으로, 여러 번 반복하여 최적의 가중치를 찾아나가는 과정

### 용어 정리

- **배치 (Batch)**: 한 번의 학습 단계에 사용되는 학습 데이터의 묶음
- **에폭 (Epoch)**: 전체 학습 데이터셋을 한 번 학습하는 과정

### 1. 배치 경사하강법 (Batch Gradient Descent)

**기울기 계산에 전체 데이터셋 사용**

- **장점**: 모든 데이터를 보고 최적을 찾음
- **단점**: 대규모 데이터셋에서는 **계산 비용이 높음**

### 2. 확률적 경사하강법 (Stochastic Gradient Descent, SGD)

**각 반복에서 무작위로 선택된 하나의 데이터 샘플 사용**

- **장점**: 배치보다 계산 속도 빠름
- **단점**: 경사의 추정이 불안정해서 학습 과정이 매끄럽지 않음

### 3. 미니배치 경사하강법 (Mini-batch Gradient Descent)

**배치 경사하강과 확률적 경사하강의 절충안**

- **장점**: 
  - 높은 메모리 효율
  - 빠른 계산 속도
  - 지역 최솟값에 갇힐 가능성 감소
- **단점**: 배치 크기 등 하이퍼파라미터에 매우 민감

---

## 출력 함수 (Output Function)

### 회귀 문제
- **출력**: 연속적인 값
- **예시**: 집값 예측, 온도 예측

### 분류 문제

#### 이진 분류
- **함수**: 시그모이드 함수
- **출력**: 0과 1 사이의 확률값

#### 다중 분류
- **함수**: 소프트맥스 함수 (Softmax)
- **출력**: 각 클래스에 대한 확률 분포

---

## 손실 함수 (Loss Function)

**모델이 실제값을 얼마나 잘 예측하는지를 측정**하는 함수

### 목표
예측값과 실제값의 차이인 손실함수의 값을 **가능한 작게** 만들어야 함

### 종류별 손실 함수

#### 1. 회귀 문제
- **MSE (Mean Squared Error, 평균제곱오차)**
- 숫자를 예측하는 문제에 사용

#### 2. 분류 문제

**이진 분류**
- **이진 교차 엔트로피 (Binary Cross Entropy)**
- 두 가지 클래스 중 하나로 분류하는 문제

**다중 분류**
- **교차 엔트로피 (Cross Entropy)**
- 세 가지 이상의 클래스 중 하나로 분류하는 문제

---

## 핵심 포인트 정리

| 개념 | 설명 | 중요성 |
|------|------|--------|
| **비선형성** | 활성화 함수로 인한 복잡한 손실 함수 지형 | 전역 최솟값 찾기 어려움 |
| **고차원성** | 많은 파라미터로 인한 과적합 위험 | 일반화 능력 저하 |
| **그래디언트 소실** | 깊은 네트워크에서 기울기 전파 문제 | 학습 효율성 저하 |
| **하이퍼파라미터** | 사용자가 설정하는 학습 관련 변수 | 모델 성능에 직접적 영향 |

---



 
