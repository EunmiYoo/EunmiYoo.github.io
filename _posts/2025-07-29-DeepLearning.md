---
title: "[Deep Learning] 딥러닝의 핵심 개념과 최적화"
excerpt: "딥러닝 모델의 최적화 방법과 핵심 개념들을 체계적으로 알아보자"

tags:
  - python
  - AI engineering
  - deep learning
  - machine learning
  - neural networks

toc: true
toc_sticky: true

date: 2025-07-29
last_modified_at: 2025-07-29
categories: 
  - deeplearning
---


## 신경망의 기본 구조

### 퍼셉트론 (Perceptron)

#### 1. 단일 퍼셉트론 (Single Perceptron)
- **구조**: 입력값에 가중치를 곱해 합산한 뒤, 임계치 이상이면 1, 미만이면 0을 출력
- **특징**: 아주 단순한 구조
- **한계**: 선형 결정 경계만 만들 수 있어서 비선형 패턴(XOR 문제)을 학습하지 못함

#### 2. 다층 퍼셉트론 (Multi-Layer Perceptron)
- **목적**: 선형 한계를 극복하기 위해 여러 개의 퍼셉트론 층을 쌓아 은닉층 도입
- **장점**:
  - 비선형 활성화 함수를 은닉층마다 적용하여 복잡한 패턴 학습 가능
  - 층을 깊게 쌓아 다차원 곡선·곡면 형태의 비선형 결정 경계 표현
  - XOR 문제뿐만 아니라 이미지·음성·텍스트의 복잡한 특성도 포착

### 신경망의 레이어 구조

신경망은 **입력층-은닉층-출력층**으로 구성되며, 은닉층의 여부에 따라 단층/다층 퍼셉트론으로 구분됩니다.

#### 은닉층의 역할
- 비선형적 데이터의 복잡한 특성이나 패턴 추출 및 학습
- 정교한 모델 구축의 핵심
- 각 층의 개별 뉴런은 가중합 연산 후 활성화 함수를 거쳐 다음 레이어로 전달

#### 손실 함수와 역전파
- **예측값 (y_pred)**와 실제값 (Y)의 차이를 MSE로 계산:
  ```
  MSE = (1/n) × Σ(1/2 × (y_pred - Y)²)
  ```
- **역전파**: 체인룰을 사용하여 각 가중치가 손실함수에 미치는 영향을 계산
- **목표**: 손실함수를 최소화하여 예측 성능 향상

---

## 순전파와 역전파

### 순전파 (Forward Propagation)
- **정의**: 입력 데이터가 각 레이어를 통과해서 출력층까지 가는 과정
- **결과**: 예측값 생성
- **목적**: 실제값과 비교하여 손실함수 계산

### 역전파 (Backpropagation)
- **목표**: 손실함수의 값을 가능한 한 최소로 줄이는 것
- **과정**:
  1. 초기 가중치는 랜덤으로 설정되어 큰 오차 존재
  2. 출력에서 발생한 오차값을 신경망을 거슬러 올라가며 계산
  3. 각 층의 가중치가 오차에 얼마나 기여하는지 계산
  4. 경사 하강법을 통해 손실함수를 최소화하는 방향으로 가중치 조절

---

##  딥러닝 모델의 최적화가 어려운 이유

### 1. 모델의 비선형성
- **활성화 함수의 비선형성**으로 인해 손실 함수의 지형이 매우 복잡해짐
- **비볼록(Non-convex) 지형** 형성
- 복잡하고 울퉁불퉁한 표면
- **전역 최솟값을 찾기 어려움**

### 2. 고차원성과 과적합
- **많은 개수의 파라미터**로 인한 차원 증가 문제
- **과적합 위험 증가**: 학습 데이터에 너무 과도하게 맞춰져서 새로운 데이터에 대한 예측 성능 저하
- 모델의 일반화 능력 저하

### 3. 그래디언트 소실 (Gradient Vanishing)
- **시그모이드 함수**의 특성으로 인한 문제
- 0과 1 사이를 벗어나면 기울기가 0에 수렴
- **연쇄 법칙(체인룰)**에 따라 여러 층을 거치면서 작은 기울기 값들이 계속 곱해짐
- 앞쪽(입력층에 가까운) 층으로 갈수록 기울기가 기하급수적으로 작아짐
- **가중치 업데이트가 거의 이루어지지 않음**

### 4. 하이퍼파라미터의 민감성
- **사용자가 사전에 설정해야 할 값**으로, 성능 최적화에 매우 중요
- 설정 변수의 적절한 조정이 최적의 모델을 위해 필수
- 잘못된 설정 시 모델 성능 크게 저하

---

## 딥러닝 모델링 시 필수적으로 고려할 사항

### 하이퍼파라미터 (Hyperparameter)

사용자가 사전에 설정해야 하는 값들입니다.

#### 1. 배치 크기 (Batch Size)
- **정의**: 한 번의 학습에 사용되는 샘플의 수
- **제약**: 메모리 사용량 때문에 배치 사이즈를 한 번에 올릴 수 없음
- **영향**: 학습 속도에 직접적인 영향

#### 2. 학습률 (Learning Rate, α)
- **정의**: 가중치를 어느 정도로 업데이트할지 결정하는 매개변수
- **공식**: `새로운 가중치 = 이전 가중치 - 학습률(α) × 기울기`
- **문제점**:
  - **너무 낮으면**: 학습이 지나치게 느려짐
  - **너무 높으면**: 손실함수가 최적값 주변에서 요동치거나 발산

#### 3. 에폭 수 (Epoch)
- **정의**: 전체 훈련 데이터셋이 네트워크를 통과하는 횟수
- **주의사항**:
  - **너무 많으면**: 학습 시간 증가, 과적합 초래
  - **너무 적으면**: 학습 부족으로 성능 저하

#### 4. 옵티마이저 (Optimizer)
- **정의**: 손실함수를 최소화하기 위해 모델의 가중치를 어떻게 업데이트할지 결정
- **종류**:
  - 확률적 경사하강법 (SGD)
  - Adam
  - Momentum

---

##  활성화 함수 (Activation Function)

뉴런에서 입력신호의 총합을 받아 출력신호로 변환하는 역할을 합니다.

### 1. 시그모이드 함수 (Sigmoid)
- **출력 범위**: 0~1
- **특징**: 초기 신경망에서 널리 사용
- **문제점**: 0과 1을 벗어난 부분에서 **그래디언트 소실 문제**가 심함

### 2. 하이퍼볼릭 탄젠트 함수 (tanh)
- **출력 범위**: -1~1로 확장
- **장점**: 학습 초기에 빠른 수렴에 도움
- **문제점**: 여전히 그래디언트 소실 문제 존재

### 3. ReLU 함수 (Rectified Linear Unit)
- **특징**: 현대 신경망에서 가장 인기 있는 함수
- **동작**:
  - **양수 입력**: 그대로 출력, 그래디언트 감소하지 않고 효과적으로 전달
  - **음수 입력**: 0을 출력, 그래디언트도 0, 가중치 업데이트 제한

### 4. Leaky ReLU 함수
- **특징**: ReLU 함수의 보완 버전
- **동작**: 음수 입력에 대해 아주 작은 기울기 유지
- **장점**: ReLU의 장점을 유지하면서 그래디언트 소실 문제 완화

---

##  경사하강법 (Gradient Descent)

최적화 알고리즘의 일종으로, 여러 번 반복하여 최적의 가중치를 찾아나가는 과정입니다.

### 용어 정리
- **배치 (Batch)**: 한 번의 학습 단계에 사용되는 학습 데이터의 묶음
- **에폭 (Epoch)**: 전체 학습 데이터셋을 한 번 학습하는 과정

### 1. 배치 경사하강법 (Batch Gradient Descent)
- **특징**: 기울기 계산에 전체 데이터셋 사용
- **장점**: 모든 데이터를 보고 최적을 찾음
- **단점**: 대규모 데이터셋에서는 **계산 비용이 높음**

### 2. 확률적 경사하강법 (Stochastic Gradient Descent, SGD)
- **특징**: 각 반복에서 무작위로 선택된 하나의 데이터 샘플 사용
- **장점**: 배치보다 계산 속도 빠름
- **단점**: 경사의 추정이 불안정해서 학습 과정이 매끄럽지 않음

### 3. 미니배치 경사하강법 (Mini-batch Gradient Descent)
- **특징**: 배치 경사하강과 확률적 경사하강의 절충안
- **장점**:
  - 높은 메모리 효율
  - 빠른 계산 속도
  - 지역 최솟값에 갇힐 가능성 감소
- **단점**: 배치 크기 등 하이퍼파라미터에 매우 민감

---

##  출력 함수 (Output Function)

### 회귀 문제
- **출력**: 연속적인 값
- **예시**: 집값 예측, 온도 예측

### 분류 문제

#### 이진 분류
- **함수**: 시그모이드 함수
- **출력**: 0과 1 사이의 확률값

#### 다중 분류
- **함수**: 소프트맥스 함수 (Softmax)
- **출력**: 각 클래스에 대한 확률 분포

---

##  손실 함수 (Loss Function)

모델이 실제값을 얼마나 잘 예측하는지를 측정하는 함수입니다.

### 목표
예측값과 실제값의 차이인 손실함수의 값을 **가능한 작게** 만들어야 합니다.

### 종류별 손실 함수

#### 1. 회귀 문제
- **MSE (Mean Squared Error, 평균제곱오차)**
- 숫자를 예측하는 문제에 사용

#### 2. 분류 문제

**이진 분류**
- **이진 교차 엔트로피 (Binary Cross Entropy)**
- 두 가지 클래스 중 하나로 분류하는 문제

**다중 분류**
- **교차 엔트로피 (Cross Entropy)**
- 세 가지 이상의 클래스 중 하나로 분류하는 문제

---


 
