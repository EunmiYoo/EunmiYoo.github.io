---
title: "위클리 페이퍼 3"
excerpt: "결정 트리의 장단점, 부스팅 기법, 차원 축소 방법에 대한 심화 학습"

tags:
  - python
  - AI engineering
  - machine learning
  - decision tree
  - boosting
  - dimensionality reduction

toc: true
toc_sticky: true

date: 2025-07-29
last_modified_at: 2025-07-29
categories: 
  - sprint
---

## 결정 트리의 장점과 단점

결정 트리는 데이터를 분류하거나 예측하기 위해 사용되는 지도학습 알고리즘

### 장점

#### 1. 쉽고 직관적인 이해 가능성

- **시각적 표현**: 계층 구조를 통해 어떤 속성이 중요한지 쉽게 파악 가능
- **투명성**: 신경망과 달리 내부 작동 방식이 명확하게 드러남
- **해석 용이성**: 의사결정 과정을 단계별로 추적 가능

#### 2. 적은 데이터 전처리 요구

- **다양한 데이터 타입 처리**: 불연속형, 연속형 데이터 모두 처리 가능
- **자동 분할**: 연속형 데이터는 임계값 기준으로 자동 분할
- **결측값 처리**: 결측값이 있어도 일정 수준까지 잘 처리 가능

#### 3. 높은 유연성과 범용성

- **다양한 문제 해결**: 회귀(Regression), 분류(Classification) 모두에 활용 가능
- **다중공선성 문제 해결**: 변수 간 상관관계에 민감하지 않음
- **효율적 속성 선택**: 적절한 속성만 선택해 분기함으로써 과도한 중복 감소

### 단점

#### 1. 과적합에 취약

**문제점:**
- 복잡한 트리는 훈련 데이터에 너무 맞춰져 새로운 데이터에 대한 일반화 성능 저하

**해결 방법:**
- **사전 가지치기 (Pre-pruning)**: 트리 성장을 조기에 멈춤
- **사후 가지치기 (Post-pruning)**: 완성된 트리에서 불필요한 가지 제거

#### 2. 높은 분산(Variance)

**문제점:**
- 입력 데이터에 조금만 변화가 있어도 트리 구조가 크게 바뀔 수 있음
- 안정성이 낮음

**해결 방법:**
- **배깅(Bagging)**: 특히 랜덤 포레스트(Random Forest) 같은 앙상블 기법 활용

#### 3. 비용 문제 (계산 자원 및 시간)

**문제점:**
- 탐욕적(Greedy) 알고리즘 사용으로 가능한 모든 분할 조건을 탐색
- 대규모 데이터셋에서는 훈련 속도와 자원 소모가 큼

---

## 부스팅 (Boosting) 앙상블 기법

부스팅은 훈련 오류를 최소화하기 위해 **약한 학습자 집합을 강한 학습자로 결합**하는 앙상블 학습 방법입니다.

### 부스팅의 핵심 원리

- **순차적 학습**: 이전 분류기의 학습 결과를 토대로 다음 분류기의 학습 데이터 샘플 가중치 조정
- **점진적 개선**: 먼저 생성된 모델을 꾸준히 개선해 나가는 방향으로 학습 진행
- **오답 가중치 부여**: 오답에 대해 높은 가중치를 부여하여 정확도 향상

### 주요 부스팅 알고리즘

#### 1. AdaBoost (Adaptive Boosting)

**특징:**
- 가장 기본적인 부스팅 알고리즘
- 오분류된 샘플에 더 높은 가중치 부여
- 각 약한 학습기의 가중치를 계산하여 최종 예측에 반영

#### 2. Gradient Boosting

**개발자:** Jerome H. Friedman (Leo Breiman의 작업을 바탕으로)

**특징:**
- **순차적 예측 변수 추가**: 앙상블에 예측 변수를 순차적으로 추가
- **잔차 오류 기반 학습**: 이전 예측 변수의 잔차 오류에 따라 훈련
- **그래디언트 하강법 결합**: 그래디언트 하강법 알고리즘과 부스팅 방법을 결합

**AdaBoost와의 차이점:**
- 데이터 포인트의 가중치를 변경하는 대신 이전 예측 변수의 잔차 오류에 따라 훈련

#### 3. XGBoost (Extreme Gradient Boosting)

**특징:**
- **고성능 구현**: 계산 속도와 규모를 위해 설계된 그래디언트 부스팅의 구현
- **병렬 처리**: CPU의 여러 코어를 활용하여 훈련 중에 병렬로 학습 가능
- **정규화**: 과적합 방지를 위한 정규화 기법 포함
- **조기 종료**: 검증 성능이 개선되지 않으면 학습을 조기에 종료

### 부스팅의 장단점

| 구분 | 내용 |
|------|------|
| **장점** | - 높은 예측 정확도<br>- 과적합에 상대적으로 강함<br>- 다양한 데이터 타입 처리 가능 |
| **단점** | - 학습 시간이 오래 걸림<br>- 해석이 어려움<br>- 하이퍼파라미터 튜닝이 복잡함 |

---

## 차원 축소 기법: 주성분 분석 vs 요인 분석

### 주성분 분석 (PCA - Principal Component Analysis)

**목적:**
- 고차원 데이터를 저차원으로 축소
- 데이터의 분산을 최대한 보존

**특징:**
- **비지도 학습**: 레이블 정보를 사용하지 않음
- **선형 변환**: 원본 변수들의 선형 조합으로 새로운 변수 생성
- **분산 기반**: 데이터의 분산을 최대화하는 방향으로 주성분 결정

**활용 분야:**
- 이미지 압축
- 노이즈 제거
- 시각화
- 특성 추출

### 요인 분석 (Factor Analysis)

**목적:**
- 관찰된 변수들을 잠재적 요인들로 설명
- 변수 간의 상관관계 구조 파악

**특징:**
- **잠재 변수 모델**: 관찰되지 않는 잠재 요인들을 추정
- **확률적 모델**: 오차 항을 포함한 확률적 구조
- **해석 가능성**: 요인들이 의미 있는 개념을 나타내도록 설계

**활용 분야:**
- 심리학 연구
- 설문조사 분석
- 사회과학 연구
- 시장 조사

### 주요 차이점

| 구분 | 주성분 분석 (PCA) | 요인 분석 (FA) |
|------|-------------------|----------------|
| **목적** | 차원 축소, 분산 보존 | 잠재 요인 발견, 변수 간 관계 설명 |
| **방법** | 선형 변환 | 확률적 모델 |
| **해석** | 수학적 최적화 | 의미적 해석 |
| **오차** | 고려하지 않음 | 오차 항 포함 |
| **유연성** | 제한적 | 더 유연한 모델링 |

---

