---
title:  "Weekly paper 3  - sprint bootcamp"
excerpt: 

tags:
  - python
  - AI engineering
  - Github

toc: true
toc_sticky: true

date: 2025-07-29
last_modified_at: 2025-07-29
categories: 
 - sprint
---


##### 결정 트리의 장점과 단점은 무엇인가요? 

[장점]
1. 쉽고 직관적인 이해 가능성

 - 시각적으로 표현되며 계층 구조를 통해 어떤 속성이 중요한지 쉽게 파악할 수 있음.

 - 신경망처럼 복잡한 알고리즘과 달리 내부 작동 방식이 명확하게 드러남.

2. 적은 데이터 전처리 요구

 - 불연속형, 연속형 데이터 모두 처리 가능.

 - 연속형 데이터는 임계값 기준으로 자동 분할.

 - 결측값이 있어도 일정 수준까지 잘 처리 가능함.

3. 높은 유연성과 범용성

 - 회귀(Regression), 분류(Classification) 모두에 활용 가능.

 - 변수 간 상관관계에 민감하지 않아 다중공선성 문제에도 강함.

 - 적절한 속성만 선택해 분기함으로써 과도한 중복을 줄임.


[단점]

1. 과적합에 취약

 - 복잡한 트리는 훈련 데이터에 너무 맞춰져, 새로운 데이터에는 일반화가 잘 안 될 수 있음.
 - 해결
    - 사전 가지치기 (pre-pruning): 트리 성장을 조기에 멈춤

    - 사후 가지치기 (post-pruning): 완성된 트리에서 불필요한 가지 제거

2. 높은 분산(Variance)

  - 입력 데이터에 조금만 변화가 있어도 트리 구조가 크게 바뀔 수 있음 → 안정성 낮음.

  - 해결 방법: 배깅(Bagging), 특히 랜덤 포레스트(Random Forest) 같은 앙상블 기법 활용. 

3. 비용 문제 (계산 자원 및 시간)

  - 트리를 구성할 때 탐욕적(Greedy) 알고리즘 사용 → 가능한 모든 분할 조건을 탐색

  - 이로 인해, 특히 대규모 데이터셋에서는 훈련 속도와 자원 소모가 상대적으로 큼



### 부스팅은 어떤 특징을 가진 앙상블 기법인가요? 토픽에서 배운 AdaBoost 이외의 부스팅 모델에는 무엇이 있는지에 대해 구글 등을 활용하여 직접 리서치해보고, 각 부스팅 모델의 특징, 장단점에 대해 말해주세요. 


부스팅은 훈련 오류를 최소화하기 위해 약한 학습자 집합을 강한 학습자로 결합하는 앙상블 학습 방법입니다. 이전 분류기의 학습 결과를 토대로 다음 분류기의 학습 데이터의 샘플 가중치를 조정해 학습을 진행하는 방법입니다. 
즉, 먼저 생성된 모델을 꾸준히 개선해 나가는 방향으로 학습이 진행되는 것입니다. 일반적으로 오답에 대해 높은 가중치를 부여하므로 정확도가 높게 나타납니다.


부스팅 알고리즘(Adaboosting 이외)

  - 그레이디언트 부스팅: Jerome H. Friedman은 Leo Breiman의 작업을 바탕으로 앙상블에 예측 변수를 순차적으로 추가하고 각 예측 변수를 이전 모델의 오류를 수정하는 방식으로 작동하는 그레이디언트 부스팅을 개발했습니다. 그러나 AdaBoost와 같은 데이터 포인트의 가중치를 변경하는 대신 그래디언트 부스팅은 이전 예측 변수의 잔차 오류에 따라 훈련합니다. 그래디언트 부스팅이라는 이름은 그래디언트 하강법 알고리즘과 부스팅 방법을 결합하기 때문에 사용되었습니다.    
  

  - 익스트림 그래디언트 부스팅 또는 XGBoost: XGBoost는 계산 속도와 규모를 위해 설계된 그래디언트 부스팅의 구현입니다. XGBoost는 CPU의 여러 코어를 활용하여 훈련 중에 병렬로 학습할 수 있습니다.  



  ####  차원 축소 기법인 주성분 분석과 요인 분석의 차이는 무엇인가?