---
title: "위클리 페이퍼 4"
excerpt: "딥러닝과 머신러닝과의 관계, 하이퍼 파라미터"

tags:
  - python
  - AI engineering
  - machine learning
  - decision tree
  - boosting
  - dimensionality reduction

toc: true
toc_sticky: true

date: 2025-08-02
last_modified_at: 2025-08-02
categories: 
  - sprint
---

## 딥러닝과 머신러닝의 관계

### 머신러닝이란?

머신러닝은 **인공지능을 만들기 위해 기계를 학습시키는 다양한 방법에 대한 학문**입니다. '로봇공학', '제어계측공학'과 같이 하나의 독립적인 학문 분야로 분류됨.

**특징:**
- 다양한 학습 방법론을 포함
- 데이터로부터 패턴을 학습하여 예측이나 분류 수행
- 전통적인 통계학과 컴퓨터 과학의 결합

### 딥러닝이란?

딥러닝(Deep Learning)은 **머신러닝보다 더 작은 개념**으로, **'신경망'을 통해 인공지능을 만드는 머신러닝의 한 종류**.

**특징:**
- 인공신경망(Artificial Neural Network) 기반
- 다층 구조를 통한 복잡한 패턴 학습
- 머신러닝의 하위 분야

### 관계 정리


![딥러닝과 머신러닝 관계](/assets/IMG_1120.WEBP)

*이미지 출처: 코드스테이츠*

```
인공지능 (AI)
    ↓
머신러닝 (Machine Learning)
    ↓
딥러닝 (Deep Learning)
```

딥러닝은 머신러닝의 한 분야이며, 머신러닝은 인공지능을 구현하는 방법 중 하나.

---

##  딥러닝 성능 향상을 위한 하이퍼 파라미터

### 하이퍼 파라미터란?

**사용자가 사전에 설정해야 하는 값**으로, 딥러닝 모델의 **성능 최적화를 위해 매우 중요한 요소**.

**중요성:**
- 설정 변수의 적절한 조정은 최적의 모델을 위해 매우 중요
- 잘못된 설정 시 모델 성능 크게 저하
- 경험과 실험을 통한 튜닝이 필요

---

## 주요 하이퍼파라미터 종류

### 1. 배치 크기 (Batch Size)

**정의**: 한 번의 학습에 사용되는 샘플의 수

**특징:**
- **메모리 사용량** 때문에 배치 사이즈를 한 번에 올릴 수 없음
- **학습 속도**에 직접적인 영향
- 너무 크면 메모리 부족, 너무 작으면 학습 불안정

### 2. 학습률 (Learning Rate, α)

**정의**: 가중치를 어느 정도로 업데이트할지 결정하는 매개변수

**공식**:
```
새로운 가중치 = 이전 가중치 - 학습률(α) × 기울기
```

**문제점**:
- **너무 낮으면**: 학습이 지나치게 느려짐
- **너무 높으면**: 손실함수가 최적값 주변에서 요동치거나 발산할 수 있어 최적화 과정 실패 가능

### 3. 에폭 수 (Epoch)

**정의**: 전체 훈련 데이터셋이 네트워크를 통과하는 횟수

**주의사항**:
- **너무 많으면**: 
  - 학습 시간이 길어짐
  - 과적합(Overfitting) 초래
- **너무 적으면**: 학습 부족으로 성능 저하

### 4. 옵티마이저 (Optimizer)

**정의**: 손실함수를 최소화하기 위해 모델의 가중치를 어떻게 업데이트할지 결정

**주요 종류**:
- **확률적 경사하강법 (SGD)**: 기본적인 최적화 방법
- **Adam**: 적응적 학습률을 사용하는 인기 있는 옵티마이저
- **Momentum**: 이전 기울기 정보를 활용하여 수렴 속도 향상

---
## 📊 하이퍼파라미터 튜닝 가이드

딥러닝이란? 
딥러닝(Deep Learning)이란 머신러닝보다 더 작은 개념으로 ‘신경망’을 통해 인공지능을 만드는 머신러닝의 한 종류


딥러닝의 성능향상을 위해 고려하는 하이퍼파라미터의 종류에는 어떤 것들이 있는가?

하이퍼 파라미터는 사용자가 사전에 설정해야할 값 , 성능최적화를 위해 중요한 값, 
설정변수의  적절한  조정은 최적의 모델을 위해 매우 중요

하이퍼 마라미터의 종류 
1. 배치 크기 (한번의 학습에 사용되는 샘플의 수, 메모리 사용량 때문에 배치 사이즈를 한번에 올릴수 없음 , 학습속도에 영향) 

2. 학습률 (알파를 의미) 가중치를 어느정도로 업데이트할지 결정
	(새로운가중치 =  이전가중치 - 학습률(알파)과 기울기 의 곱)

학습률이 너무 낮으면 학습이 지나치게 느려짐
반면 학습률이 너무 높으면 손실함수가 최적값 주변에서 요동치거나 발산할수 있어 최적화 과정 실패할수 있음 

3. 에폭 수 : 전체 훈련 데이터셋이 네트워크 통과하는 횟수
에폭수가 너무 많으면 학습 길어짐… 과적합 초래… 

4. 옵티마이저:  손실함수를 최소화 하기위해 모델의 가중치를 어떻게 업데이트 할지 결정, 확률적 경사하강법, 아담, 모멘텀 등이 있다고 합니다… 
