---
title:  "Weekly paper 2 - sprint bootcamp"
excerpt: 

tags:
  - python
  - AI engineering
  - Github

toc: true
toc_sticky: true

date: 2025-07-21
last_modified_at: 2025-07-21
categories: 
 - sprint
---
##### 1. 지도학습과 비지도학습의 차이는 무엇인가요?
----
머신러닝에서 프로그램은 ‘학습(Learning)’을 하게 됩니다. 이 때, 학습을 하는 방법에 따라 크게 지도학습과 비지도학습으로 나뉠 수 있습니다.  

a. 지도 학습

정답이 있는 데이터(레이블링된 데이터)를 사용하여 모델을 훈련시키는 학습 방법

    *예시: 분류(예: 스팸 메일 분류), 회귀(예: 집값 예측)*

b. 비지도 학습

정답 없이 데이터 자체의 패턴이나 구조를 발견하도록 학습시키는 방법.
       
    *예시: 클러스터링(예: 고객 분류), 차원 축소(예: 데이터 시각화), 연관 규칙 학습(예: 장바구니 분석)*
  


##### 2. 손실 함수(loss function)란 무엇이며, 왜 중요한가요?
---

손실함수는 머신러닝이나 딥러닝 모델이 예측한 값과 실제 값 사이의 차이를 측정하는 함수.
이를 통해 모델의 성능을 평가하고, 어떤 방향으로 개선되어야 할지 알려주는 역할을 함. 손실함수의 값을 최소화하는 것이 모델 학습의 목표.
따라서 손실 함수 없이는 모델이 자신의 성능을 개선해 나가는 것이 불가능. 손실 함수는 모델 학습의 가이드라인을 제공함으로써, 최적화 과정에서 모델이 올바른 방향으로 나아갈 수 있도록 도움.



##### 3. 모델 학습 시 발생할 수 있는 편향과 분산에 대해 설명하고, 두 개념의 관계에 대해 설명해 주세요.
---

- `편향(Bias)`
추정 결과가 한 쪽으로 치우치는 경향을 보임으로써 발생하는 오차로써 모델에서는 예측한 결과가정답과 일정하게 차이가 나는 정도를 의미함 
</br>
- `분산(Variance)`
변량(데이터)들이 퍼져있는 정도를 의미함. 모델에서는 주어진 데이터 포인트에 대한 모델예측의 가변성을 뜻함
</br>
- `편향과 분산은 상충관계(trade-off)`를 갖는데 서로 상반되는 특성을 가지고 있어, 한쪽을 줄이려고 하면 다른 쪽이 증가하는 경향이 있음. 따라서 모델을 학습할 때는 편향과 분산의 균형을 잘 맞춰야 함. 편향이 높고 분산이 낮은 모델은 단순하지만 예측력이 낮고, 분산이 높고 편향이 낮은 모델은 복잡하지만 과대적합될 위험이 있음. 이상적인 모델은 편향과 분산이 모두 낮은 모델. 이러한 모델은 학습 데이터와 새로운 데이터모두에서 좋은 예측 성능을 보임.



##### 4. K-폴드 교차 검증에서 K의 값을 선택할 때 고려해야 할 점은 무엇인가요?
---

1. K-폴드 교차 검증은 편향된 모델을 해결하기 위해서 교차검증의 방법을 이용하여 다양한 학습과 평가를 수행하는 데 있어서 가장 보편적으로 사용되는 교차 검증기법임. 제한된 데이터 샘플에서 머신러닝 모델을 평가하는 데 사용되는 리샘플링 절차
</br>
2. 방법
 - 주어진 데이터를 K개의 폴드로 나누어, 각 폴드를 한 번씩 검증 세트로 사용하고 나머지를 학습 세트로 활용하는 방식으로 모델을 평가하는 방법. 전체 데이터를 고르게 나누어 반복적으로 학습 및 검증을 수행하기 때문에, 데이터의 특정한 분할에 의한 편향을 줄이고 모델의 일반화 성능을 더 정확하게 평가할 수 있음.
</br>

3. 고려해야 할 점:
 - 계산 시간:
    - K가 커질수록(예: 10개로 나눌 때) 계산 시간이 오래 걸림. 반대로 K가 작을수록(예: 3개로 나눌 때) 시간이 덜 걸림.

 - 정확도와 안정성:
    - K가 클수록: 결과가 더 정확하지만, 결과들이 들쭉날쭉 불안정할 수 있음.
    - K가 작을수록: 결과가 덜 정확할 수 있지만, 비교적 안정적.
 - 데이터 양:  
    - 데이터가 적으면 K를 크게 해서 데이터를 최대한 활용하는 게 좋고, 데이터가 많으면 K를 작게 해도 충분함. 일반적으로는 K=5나 K=10을 가장 많이 사용.


